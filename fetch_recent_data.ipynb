{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import yfinance as yf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, date_format"
   ],
   "id": "b2d3eb85a7686c8a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define tickers, date range and output directory\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'V']\n",
    "start_date = '2022-12-13'\n",
    "yesterday = datetime.now() - timedelta(1)\n",
    "end_date = yesterday.strftime('%Y-%m-%d')\n",
    "HDFS_PATH = 'hdfs://10.84.129.52:9000/trab/g05'\n",
    "output_dir = HDFS_PATH + '/data/'"
   ],
   "id": "5a3afeea5b467e6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fetch recent data and save to CSV\n",
    "def fetch_recent_data():\n",
    "    for ticker in TICKERS:\n",
    "        # Fetch data using yfinance\n",
    "        data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "        # Convert the pandas DataFrame to a Spark DataFrame\n",
    "        data_spark = spark.createDataFrame(data.reset_index())\n",
    "\n",
    "        # Convert the 'Date' column to a date type and format it as \"dd-MM-yyyy\"\n",
    "        data_spark = data_spark.withColumn('Date', date_format(to_date('Date', 'yyyy-MM-dd'), 'dd-MM-yyyy'))\n",
    "\n",
    "        # Reorder columns\n",
    "        columns_order = ['Date', 'Low', 'Open', 'Volume', 'High', 'Close', 'Adj Close']\n",
    "        data_spark = data_spark.select(columns_order)\n",
    "\n",
    "        # Rename columns to match the required format\n",
    "        data_spark = data_spark.withColumnRenamed('Adj Close', 'Adjusted Close')\n",
    "\n",
    "        # Save to CSV\n",
    "        output_file = output_dir + ticker + '/recent_data.csv'\n",
    "        data_spark.write.csv(output_file, header=True, mode='overwrite')\n",
    "        print(f\"Saved {ticker} file\")"
   ],
   "id": "7347f4d9ea8109ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "fetch_recent_data()",
   "id": "8bcf7fd5236bd7c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "spark.stop()",
   "id": "c54becc5f6a0117"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

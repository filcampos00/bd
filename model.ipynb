{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, when, isnan, col, lag, to_date, current_date, date_sub\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ],
   "id": "7d028821e27a9e55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# constants\n",
    "HDFS_PATH = 'hdfs://10.84.129.52:9000/trab/g05'\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'V']\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"Volume\", LongType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Adjusted_Close\", DoubleType(), True)\n",
    "])"
   ],
   "id": "606d83dd679e1d0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataframes = {}\n",
    "\n",
    "\n",
    "def check_missing_values(df, ticker):\n",
    "    missing_values = df.select(\n",
    "        [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    print(f\"Missing values for {ticker}:\")\n",
    "    missing_values.show()\n",
    "\n",
    "\n",
    "# Read the data from all CSV files in the directory for each ticker\n",
    "for ticker in TICKERS:\n",
    "    dataframes[ticker] = spark.read.csv(f'{HDFS_PATH}/data/{ticker}/*.csv', header=True, schema=SCHEMA)\n",
    "    check_missing_values(dataframes[ticker], ticker)"
   ],
   "id": "f056f8cb73a05eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_data(df):\n",
    "    train_size = int(df.count() * 0.75)\n",
    "    train_set = df.limit(train_size)\n",
    "    test_set = df.subtract(train_set)\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "# Preprocess the data, add new feature, split the data into training and testing sets, save scaler model\n",
    "def preprocess_data(ticker):\n",
    "    df = dataframes[ticker]\n",
    "\n",
    "    # Convert 'Date' column to datetime\n",
    "    df = df.withColumn(\"Date\", to_date(df[\"Date\"], 'dd-MM-yyyy'))\n",
    "\n",
    "    # Filter data from the last 10 years\n",
    "    df = df.filter(df[\"Date\"] >= date_sub(current_date(), 365 * 10))\n",
    "\n",
    "    # Sort the data by 'Date'\n",
    "    df = df.orderBy(\"Date\")\n",
    "\n",
    "    # Drop 'Adjusted_Close' column\n",
    "    df = df.drop('Adjusted_Close')\n",
    "\n",
    "    # Create a new feature: difference between 'Close' and 'Open'\n",
    "    df = df.withColumn('Close_Open_Diff', df['Close'] - df['Open'])\n",
    "\n",
    "    # Add a new column 'Prev_Close' with the previous day's 'Close' price\n",
    "    window_spec = Window.orderBy(\"Date\")\n",
    "    df = df.withColumn(\"Prev_Close\", lag(\"Close\").over(window_spec))\n",
    "    df = df.na.drop()\n",
    "\n",
    "    # Assemble the features into a feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"Low\", \"Open\", \"Volume\", \"High\", \"Close_Open_Diff\", \"Prev_Close\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_set, test_set = split_data(df)\n",
    "\n",
    "    # Normalize the features with MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scaler_model = scaler.fit(train_set)\n",
    "    scaled_train_set = scaler_model.transform(train_set)\n",
    "    scaled_test_set = scaler_model.transform(test_set)\n",
    "\n",
    "    # Save the MinMaxScaler model\n",
    "    scaler_model.write().overwrite().save(f'{HDFS_PATH}/models/{ticker}/{ticker}_scaler')\n",
    "\n",
    "    return scaled_train_set, scaled_test_set"
   ],
   "id": "1eac9ce9755e912"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predictions_dict = {}\n",
    "\n",
    "\n",
    "def cross_validate_model(model, train_set):\n",
    "    # Define the parameter grid\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(model.regParam, [0.1, 0.01]) \\\n",
    "        .addGrid(model.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "\n",
    "    # Define the cross-validation\n",
    "    crossval = CrossValidator(estimator=model,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=RegressionEvaluator(labelCol=\"Close\", predictionCol=\"prediction\"),\n",
    "                              numFolds=10)  # use 10-fold cross-validation\n",
    "\n",
    "    # Train the model\n",
    "    cvModel = crossval.fit(train_set)\n",
    "\n",
    "    return cvModel\n",
    "\n",
    "\n",
    "# Train the model and make predictions\n",
    "def train_predict(train_set, test_set, ticker):\n",
    "    # Define the model\n",
    "    model = LinearRegression(featuresCol='scaledFeatures', labelCol='Close')\n",
    "\n",
    "    # Call the cross_validate_model function\n",
    "    cvModel = cross_validate_model(model, train_set)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = cvModel.transform(test_set)\n",
    "\n",
    "    # Store predictions in the dictionary (used to plot the charts later)\n",
    "    predictions_dict[ticker] = predictions\n",
    "\n",
    "    # Evaluate the best model\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Close\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "    print(f\"Evaluation metrics for {ticker}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"R2: {r2}\\n\")\n",
    "\n",
    "    # Save the model\n",
    "    cvModel.bestModel.write().overwrite().save(f'{HDFS_PATH}/models/{ticker}/{ticker}_model')"
   ],
   "id": "5772de32e9fe9883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_predictions(ticker):\n",
    "    # Get the predictions for the ticker\n",
    "    predictions = predictions_dict[ticker]\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    predictions_pd = predictions.select(\"Date\", \"Close\", \"prediction\").toPandas()\n",
    "\n",
    "    # Set 'Date' as the index of the DataFrame\n",
    "    predictions_pd.set_index('Date', inplace=True)\n",
    "\n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(predictions_pd['Close'], label='Actual')\n",
    "    plt.plot(predictions_pd['prediction'], label='Predicted')\n",
    "    plt.title(f'Actual vs Predicted Close Prices for {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "ea2a8dfd4c7ab4df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for ticker in TICKERS:\n",
    "    train_set, test_set = preprocess_data(ticker)\n",
    "    train_predict(train_set, test_set, ticker)\n",
    "    plot_predictions(ticker)"
   ],
   "id": "c0e698dc08df3b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "spark.stop()",
   "id": "d72a732312744751"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression, DecisionTreeRegressor, RandomForestRegressor, \\\n",
    "    GBTRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, when, isnan, col, lag, to_date, current_date, date_sub\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "from pyspark.sql.window import Window"
   ],
   "id": "7d028821e27a9e55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# constants\n",
    "HDFS_PATH = 'hdfs://10.84.129.52:9000/trab/g05'\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'V']\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"Volume\", LongType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Adjusted_Close\", DoubleType(), True)\n",
    "])\n",
    "MODELS = {\n",
    "    'LinearRegression': LinearRegression(featuresCol='scaledFeatures', labelCol='Next_Day_Close'),\n",
    "    'GeneralizedLinearRegression': GeneralizedLinearRegression(featuresCol='scaledFeatures',\n",
    "                                                               labelCol='Next_Day_Close'),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(featuresCol='scaledFeatures', labelCol='Next_Day_Close'),\n",
    "    'RandomForestRegressor': RandomForestRegressor(featuresCol='scaledFeatures', labelCol='Next_Day_Close'),\n",
    "    'GBTRegressor': GBTRegressor(featuresCol='scaledFeatures', labelCol='Next_Day_Close')\n",
    "}"
   ],
   "id": "606d83dd679e1d0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataframes = {}\n",
    "\n",
    "def check_missing_values(df, ticker):\n",
    "    missing_values = df.select(\n",
    "        [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    print(f\"Missing values for {ticker}:\")\n",
    "    missing_values.show()\n",
    "\n",
    "# Read the data from all CSV files in the directory for each ticker\n",
    "for ticker in TICKERS:\n",
    "    dataframes[ticker] = spark.read.csv(HDFS_PATH + '/data/' + ticker + '/*.csv', header=True, schema=SCHEMA)\n",
    "    check_missing_values(dataframes[ticker], ticker)"
   ],
   "id": "f056f8cb73a05eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_data(ticker):\n",
    "    df = dataframes[ticker]\n",
    "\n",
    "    # Convert 'Date' column to datetime\n",
    "    df = df.withColumn(\"Date\", to_date(df[\"Date\"], 'dd-MM-yyyy'))\n",
    "\n",
    "    # Filter data from the last 10 years\n",
    "    df = df.filter(df[\"Date\"] >= date_sub(current_date(), 365 * 10))\n",
    "\n",
    "    # Drop 'Adjusted_Close' column\n",
    "    df = df.drop('Adjusted_Close')\n",
    "\n",
    "    # Shift the 'Close' column up by one row to make the model predict for the next day\n",
    "    window = Window.orderBy(df['Date'])\n",
    "    df = df.withColumn('Next_Day_Close', lag(df['Close'], -1).over(window))\n",
    "\n",
    "    # Drop the rows with null values that result from the shift\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Assemble the features into a feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"Low\", \"Open\", \"Volume\", \"High\", \"Close\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    # Normalize the features with MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(df)\n",
    "    df = scalerModel.transform(df)\n",
    "\n",
    "    # Update the DataFrame in the dictionary\n",
    "    dataframes[ticker] = df"
   ],
   "id": "1eac9ce9755e912"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_models = {}\n",
    "predictions_dict = {}\n",
    "\n",
    "# Save the model and overwrite if it already exists\n",
    "def save_model(model, ticker):\n",
    "    model.write().overwrite().save(HDFS_PATH + '/models/' + ticker + '_model')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "def split_data(df):\n",
    "    train_size = int(df.count() * 0.75)\n",
    "    train_data = df.limit(train_size)\n",
    "    test_data = df.subtract(train_data)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# Train the models and make predictions\n",
    "def train_predict(ticker):\n",
    "    df = dataframes[ticker]\n",
    "\n",
    "    train_data, test_data = split_data(df)\n",
    "\n",
    "    best_model_name = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for model_name, model in MODELS.items():\n",
    "        # Define cross-validation\n",
    "        crossval = CrossValidator(estimator=model,\n",
    "                                  estimatorParamMaps=ParamGridBuilder().build(),  # empty parameter grid\n",
    "                                  evaluator=RegressionEvaluator(labelCol=\"Next_Day_Close\", predictionCol=\"prediction\"),\n",
    "                                  numFolds=10)  # 10-fold cross-validation\n",
    "\n",
    "        # Run cross-validation, and choose the best set of parameters\n",
    "        cvModel = crossval.fit(train_data)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions = cvModel.transform(test_data)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"Next_Day_Close\", predictionCol=\"prediction\")\n",
    "        rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_name = model_name\n",
    "            best_models[ticker] = cvModel.bestModel\n",
    "\n",
    "    # Make predictions on the test data with the best model\n",
    "    predictions = best_models[ticker].transform(test_data)\n",
    "\n",
    "    # Store the predictions in the dictionary\n",
    "    predictions_dict[ticker] = predictions\n",
    "\n",
    "    # Evaluate the best model\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Next_Day_Close\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "    print(f\"Best model for {ticker} is {best_model_name} with performance:\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"R2: {r2}\\n\")\n",
    "\n",
    "    # Save the best model\n",
    "    save_model(best_models[ticker], ticker)"
   ],
   "id": "5772de32e9fe9883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_predictions(ticker):\n",
    "    # Get the predictions for the ticker\n",
    "    predictions = predictions_dict[ticker]\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    predictions_pd = predictions.select(\"Date\", \"Next_Day_Close\", \"prediction\").toPandas()\n",
    "\n",
    "    # Convert 'Date' column to datetime format\n",
    "    predictions_pd['Date'] = pd.to_datetime(predictions_pd['Date'])\n",
    "\n",
    "    # Set 'Date' as the index of the DataFrame\n",
    "    predictions_pd.set_index('Date', inplace=True)\n",
    "\n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(predictions_pd['Next_Day_Close'], label='Actual')\n",
    "    plt.plot(predictions_pd['prediction'], label='Predicted')\n",
    "    plt.title(f'Actual vs Predicted Close Prices for {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "ea2a8dfd4c7ab4df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for ticker in TICKERS:\n",
    "    preprocess_data(ticker)\n",
    "    train_predict(ticker)\n",
    "    plot_predictions(ticker)"
   ],
   "id": "c0e698dc08df3b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ],
   "id": "d72a732312744751"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

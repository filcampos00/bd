{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScalerModel\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date\n",
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "HDFS_PATH = 'hdfs://10.84.129.52:9000/trab/g05'\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'V']\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'g05in',\n",
    "    bootstrap_servers='10.204.131.11:9092',\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8'))  # reads the message as JSON\n",
    ")\n",
    "\n",
    "# Load the saved models\n",
    "models = {ticker: LinearRegressionModel.load(f'{HDFS_PATH}/models/{ticker}/{ticker}_model') for ticker in TICKERS}\n",
    "scalers = {ticker: MinMaxScalerModel.load(f'{HDFS_PATH}/models/{ticker}/{ticker}_scaler') for ticker in TICKERS}"
   ],
   "id": "a6a0aebd8aa12afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_real_time_data(ticker, data):\n",
    "    # Use the respective MinMaxScaler model\n",
    "    scaler_model = scalers[ticker]\n",
    "\n",
    "    # Transform the data into the format expected by the model\n",
    "    features = [data['Low'], data['Open'], data['Volume'], data['High'], data['Close']]\n",
    "    df = spark.createDataFrame([features], [\"Low\", \"Open\", \"Volume\", \"High\", \"Close\"])\n",
    "\n",
    "    # Add 'Date' column with current date and format it to 'dd-MM-yyyy'\n",
    "    df = df.withColumn(\"Date\", current_date())\n",
    "    df = df.withColumn(\"Date\", date_format(df[\"Date\"], 'dd-MM-yyyy'))\n",
    "\n",
    "    # Reorder the columns\n",
    "    df = df.select(['Date', 'Low', 'Open', 'Volume', 'High', 'Close'])\n",
    "\n",
    "    # Create a new feature: difference between 'Close' and 'Open'\n",
    "    df = df.withColumn('Close_Open_Diff', df['Close'] - df['Open'])\n",
    "\n",
    "    # Assemble the features into a feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"Low\", \"Open\", \"Volume\", \"High\", \"Close_Open_Diff\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    # Normalize the features with MinMaxScaler\n",
    "    scaled_df = scaler_model.transform(df)\n",
    "\n",
    "    return scaled_df"
   ],
   "id": "745f4954ce350153"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def send_email(subject, body):\n",
    "    to=\"pytest571@gmail.com\"\n",
    "    gmail_user=\"pytest571@gmail.com\"\n",
    "    gmail_pwd=\"srva zsjz tjkk yvgq\"\n",
    "    msg = MIMEText(body)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = gmail_user\n",
    "    msg['To'] = to\n",
    "\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()\n",
    "    server.login(gmail_user, gmail_pwd)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ],
   "id": "73c18410b6e97702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    # Consume messages from Kafka\n",
    "    for message in consumer:\n",
    "        # Convert the JSON string to a dictionary\n",
    "        message_dict = json.loads(message.value)\n",
    "\n",
    "        # Iterate over the messages\n",
    "        for ticker, data in message_dict.items():\n",
    "            # Preprocess the real-time data\n",
    "            preprocessed_features_df = preprocess_real_time_data(ticker, data)\n",
    "\n",
    "            # Use the model to make a prediction\n",
    "            prediction = models[ticker].transform(preprocessed_features_df).select(\"prediction\").first()[0]\n",
    "\n",
    "            # Compare the predicted value with the real-time value\n",
    "            real_time_value = data['Close']\n",
    "            difference = real_time_value - prediction\n",
    "\n",
    "            # Calculate the percentage difference\n",
    "            percentage_difference = abs(difference / real_time_value) * 100\n",
    "\n",
    "            # If the percentage difference is 10% or more, send an email\n",
    "            if percentage_difference >= 10:\n",
    "                subject = f\"Alert: Buy/Sell opportunity for {ticker} stocks\"\n",
    "                body = f\"For {ticker}, the real-time value is {real_time_value}, the predicted value is {prediction}, and the difference is {difference} ({percentage_difference}%).\"\n",
    "                send_email(subject, body)\n",
    "\n",
    "            # Print the result\n",
    "            print(\n",
    "                f\"For {ticker}, the real-time value is {real_time_value}, the predicted value is {prediction}, and the difference is {difference} ({percentage_difference}%).\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    consumer.close()\n",
    "    spark.stop()"
   ],
   "id": "6194e8d102adebbe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

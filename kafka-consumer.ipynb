{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScalerModel\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "HDFS_PATH = 'hdfs://10.84.129.52:9000/trab/g05'\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'V']\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'g05in',  # Kafka topic to consume from\n",
    "    bootstrap_servers='10.204.131.11:9092',  # List of brokers\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8'))  # Deserializer function for the messages\n",
    ")\n",
    "\n",
    "# Load the saved models\n",
    "models = {ticker: LinearRegressionModel.load(f'{HDFS_PATH}/models/{ticker}/{ticker}_model') for ticker in TICKERS}\n",
    "scalers = {ticker: MinMaxScalerModel.load(f'{HDFS_PATH}/models/{ticker}/{ticker}_scaler') for ticker in TICKERS}"
   ],
   "id": "a6a0aebd8aa12afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_real_time_data(df, ticker):\n",
    "    # Use the saved MinMaxScaler model\n",
    "    scaler_model = scalers[ticker]\n",
    "\n",
    "    # TODO: fix date column problem\n",
    "    # Convert 'Date' column to datetime if it's not already in the datetime format\n",
    "    df = df.withColumn(\"Date\", to_date(df[\"Date\"], 'dd-MM-yyyy'))\n",
    "\n",
    "    # Create a new feature: difference between 'Close' and 'Open'\n",
    "    df = df.withColumn('Close_Open_Diff', df['Close'] - df['Open'])\n",
    "\n",
    "    # Add a new column 'Prev_Close' with the previous day's 'Close' price\n",
    "    window_spec = Window.orderBy(\"Date\")\n",
    "    df = df.withColumn(\"Prev_Close\", lag(\"Close\").over(window_spec))\n",
    "    df = df.na.drop()\n",
    "\n",
    "    # Assemble the features into a feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"Low\", \"Open\", \"Volume\", \"High\", \"Close_Open_Diff\", \"Prev_Close\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    # Normalize the features with MinMaxScaler\n",
    "    scaled_df = scaler_model.transform(df)\n",
    "\n",
    "    return scaled_df"
   ],
   "id": "745f4954ce350153"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    # Consume messages\n",
    "    for message in consumer:\n",
    "        # Convert the JSON string to a dictionary\n",
    "        message_dict = json.loads(message.value)\n",
    "\n",
    "        # Parse the inner JSON\n",
    "        for ticker, data in message_dict.items():\n",
    "            # Transform the data into the format expected by the model\n",
    "            features = [data['Low'], data['Open'], data['Volume'], data['High'], data['Close']]\n",
    "            features_df = spark.createDataFrame([features], [\"Low\", \"Open\", \"Volume\", \"High\", \"Close\"])\n",
    "\n",
    "            # Preprocess the real-time data\n",
    "            preprocessed_features_df = preprocess_real_time_data(features_df, ticker)\n",
    "\n",
    "            # Use the model to make a prediction\n",
    "            prediction = models[ticker].transform(preprocessed_features_df).select(\"prediction\").first()[0]\n",
    "\n",
    "            # Compare the predicted value with the real-time value\n",
    "            real_time_value = data['Close']\n",
    "            difference = real_time_value - prediction\n",
    "\n",
    "            # Print the result\n",
    "            print(\n",
    "                f\"For {ticker}, the real-time value is {real_time_value}, the predicted value is {prediction}, and the difference is {difference}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    consumer.close()\n",
    "    spark.stop()"
   ],
   "id": "73c18410b6e97702"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
